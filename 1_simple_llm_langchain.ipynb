{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 1 - Simple LLM call using langchain\n",
    "This is the basic lesson to show you how to invoke an LLM using langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# We first install the packages\n",
    "%pip install langchain langchain-openai langchain-community --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We install our envirnment variables helper library\n",
    "%pip install python-dotenv --quiet\n",
    "\n",
    "# We load the library and the environment variables\n",
    "import dotenv\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write me a shell script to list the files in the current directory. \n",
      "Output the script as markdown code block\n"
     ]
    }
   ],
   "source": [
    "# We load the settings\n",
    "import _settings\n",
    "\n",
    "# The challenge prompt is the challenge we want our agents to solve\n",
    "# You can change it in the _settings.py file\n",
    "# Be sure to restart ^^ your kernel after changing the settings\n",
    "challenge_prompt = _settings.CHALLENGE_PROMPT\n",
    "print(challenge_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "```\n",
      "#!/bin/bash\n",
      "\n",
      "# This script lists all the files in the current directory\n",
      "\n",
      "echo \"Files in current directory:\"\n",
      "\n",
      "ls # lists all the files in the current directory\n",
      "\n",
      "# To make the output more readable, we can use the -l flag to display the files in a list format\n",
      "\n",
      "echo \"Files in current directory (in list format):\"\n",
      "\n",
      "ls -l # lists all the files in the current directory in a list format\n",
      "\n",
      "# To make the output more detailed, we can use the -a flag to display all files, including hidden ones\n",
      "\n",
      "echo \"All files in current directory (including hidden files):\"\n",
      "\n",
      "ls -a # lists all files in the current directory, including hidden files\n",
      "\n",
      "# To display only the files with a specific extension, we can use the * wildcard character\n",
      "\n",
      "echo \"Files with .txt extension:\"\n",
      "\n",
      "ls *.txt # lists all files in the current directory with a .txt extension\n",
      "\n",
      "# To save the output to a file, we can use the > symbol and specify the file name\n",
      "\n",
      "ls > file_list.txt # saves the list of files in the current directory to a file named \"file_list.txt\"\n",
      "\n",
      "# To display the contents of the saved file, we can use the cat command\n",
      "\n",
      "echo \"Contents of file\n"
     ]
    }
   ],
   "source": [
    "# We load the llm language model from _models\n",
    "from langchain_openai import OpenAI\n",
    "llm = OpenAI()\n",
    "\n",
    "# We invoke the language model with the challenge prompt\n",
    "result = llm.invoke(_settings.CHALLENGE_PROMPT)\n",
    "\n",
    "# We print the result\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see what is going of the wire to the LLM and back, we add a callback handler that prints this out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============\n",
      "[llm][start] - prompts: Write me a shell script to list the files in the current directory. \n",
      "Output the script as markdown code block\n",
      "[llm][new_token] \n",
      "\n",
      "\n",
      "[llm][new_token] ``\n",
      "[llm][new_token] `\n",
      "\n",
      "[llm][new_token] #!/\n",
      "[llm][new_token] bin\n",
      "[llm][new_token] /bash\n",
      "[llm][new_token] \n",
      "\n",
      "\n",
      "[llm][new_token] #\n",
      "[llm][new_token]  This\n",
      "[llm][new_token]  script\n",
      "[llm][new_token]  lists\n",
      "[llm][new_token]  all\n",
      "[llm][new_token]  the\n",
      "[llm][new_token]  files\n",
      "[llm][new_token]  in\n",
      "[llm][new_token]  the\n",
      "[llm][new_token]  current\n",
      "[llm][new_token]  directory\n",
      "[llm][new_token] \n",
      "\n",
      "\n",
      "[llm][new_token] #\n",
      "[llm][new_token]  Change\n",
      "[llm][new_token]  directory\n",
      "[llm][new_token]  to\n",
      "[llm][new_token]  current\n",
      "[llm][new_token]  directory\n",
      "[llm][new_token] \n",
      "\n",
      "[llm][new_token] cd\n",
      "[llm][new_token]  .\n",
      "\n",
      "\n",
      "[llm][new_token] #\n",
      "[llm][new_token]  Loop\n",
      "[llm][new_token]  through\n",
      "[llm][new_token]  all\n",
      "[llm][new_token]  files\n",
      "[llm][new_token]  in\n",
      "[llm][new_token]  current\n",
      "[llm][new_token]  directory\n",
      "[llm][new_token] \n",
      "\n",
      "[llm][new_token] for\n",
      "[llm][new_token]  file\n",
      "[llm][new_token]  in\n",
      "[llm][new_token]  *\n",
      "\n",
      "[llm][new_token] do\n",
      "[llm][new_token] \n",
      "\n",
      "[llm][new_token]    \n",
      "[llm][new_token]  #\n",
      "[llm][new_token]  Check\n",
      "[llm][new_token]  if\n",
      "[llm][new_token]  file\n",
      "[llm][new_token]  is\n",
      "[llm][new_token]  a\n",
      "[llm][new_token]  regular\n",
      "[llm][new_token]  file\n",
      "[llm][new_token] \n",
      "\n",
      "[llm][new_token]    \n",
      "[llm][new_token]  if\n",
      "[llm][new_token]  [\n",
      "[llm][new_token]  -\n",
      "[llm][new_token] f\n",
      "[llm][new_token]  \"$\n",
      "[llm][new_token] file\n",
      "[llm][new_token] \"\n",
      "[llm][new_token]  ]\n",
      "\n",
      "[llm][new_token]    \n",
      "[llm][new_token]  then\n",
      "[llm][new_token] \n",
      "\n",
      "[llm][new_token]        \n",
      "[llm][new_token]  #\n",
      "[llm][new_token]  Print\n",
      "[llm][new_token]  file\n",
      "[llm][new_token]  name\n",
      "[llm][new_token] \n",
      "        echo \"$file\"\n",
      "    fi\n",
      "done\n",
      "```\n",
      "\n",
      "\n",
      "[llm][new_token] To\n",
      "[llm][new_token]  run\n",
      "[llm][new_token]  this\n",
      "[llm][new_token]  script\n",
      "[llm][new_token] ,\n",
      "[llm][new_token]  save\n",
      "[llm][new_token]  it\n",
      "[llm][new_token]  as\n",
      "[llm][new_token]  a\n",
      "[llm][new_token]  .\n",
      "[llm][new_token] sh\n",
      "[llm][new_token]  file\n",
      "[llm][new_token]  and\n",
      "[llm][new_token]  run\n",
      "[llm][new_token]  it\n",
      "[llm][new_token]  in\n",
      "[llm][new_token]  the\n",
      "[llm][new_token]  terminal\n",
      "[llm][new_token]  using\n",
      "[llm][new_token]  the\n",
      "[llm][new_token]  command\n",
      "[llm][new_token]  `\n",
      "[llm][new_token] ./\n",
      "[llm][new_token] script\n",
      "[llm][new_token] .sh\n",
      "[llm][new_token] `.\n",
      "[llm][new_token] \n",
      "\n",
      "=============\n",
      "[llm][end] - generation \n",
      "\n",
      "```\n",
      "#!/bin/bash\n",
      "\n",
      "# This script lists all the files in the current directory\n",
      "\n",
      "# Change directory to current directory\n",
      "cd .\n",
      "\n",
      "# Loop through all files in current directory\n",
      "for file in *\n",
      "do\n",
      "    # Check if file is a regular file\n",
      "    if [ -f \"$file\" ]\n",
      "    then\n",
      "        # Print file name\n",
      "        echo \"$file\"\n",
      "    fi\n",
      "done\n",
      "```\n",
      "\n",
      "To run this script, save it as a .sh file and run it in the terminal using the command `./script.sh`.\n"
     ]
    }
   ],
   "source": [
    "# We import the callback handler from _callbacks\n",
    "from _callbacks import PrettyPrintCallbackHandler\n",
    "callback = PrettyPrintCallbackHandler()\n",
    "\n",
    "# Now we can do the same call\n",
    "# but this time we will use the callback handler\n",
    "# That will print out the intermediate results\n",
    "# We set the streaming parameter to True to see how the callback works\n",
    "\n",
    "# To make sure we always the same result we set the temperature to 0\n",
    "# Think of reducing the creative randomness of the llm\n",
    "llm = OpenAI(streaming=True, temperature=0 , callbacks=[callback])\n",
    "result=llm.invoke(challenge_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "```\n",
      "#!/bin/bash\n",
      "\n",
      "# This script lists all the files in the current directory\n",
      "\n",
      "# Change directory to current directory\n",
      "cd .\n",
      "\n",
      "# Loop through all files in current directory\n",
      "for file in *\n",
      "do\n",
      "    # Check if file is a regular file\n",
      "    if [ -f \"$file\" ]\n",
      "    then\n",
      "        # Print file name\n",
      "        echo \"$file\"\n",
      "    fi\n",
      "done\n",
      "```\n",
      "\n",
      "To run this script, save it as a .sh file and run it in the terminal using the command `./script.sh`.\n"
     ]
    }
   ],
   "source": [
    "# We print the final result\n",
    "# And get the same result as before\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4\n",
      "[chat_model][start] - prompts : Write me a shell script to list the files in the current directory. \n",
      "Output the script as markdown code block\n",
      "\n",
      "=============\n",
      "[llm][end] - generation Here is the shell script you requested:\n",
      "\n",
      "```bash\n",
      "#!/bin/bash\n",
      "# This script will list all files in the current directory\n",
      "\n",
      "for file in ./*\n",
      "do\n",
      "  if [ -f \"$file\" ]; then\n",
      "    echo \"$file\"\n",
      "  fi\n",
      "done\n",
      "```\n",
      "\n",
      "This script will loop through all items in the current directory and print out the name of each item that is a file.\n",
      "content='Here is the shell script you requested:\\n\\n```bash\\n#!/bin/bash\\n# This script will list all files in the current directory\\n\\nfor file in ./*\\ndo\\n  if [ -f \"$file\" ]; then\\n    echo \"$file\"\\n  fi\\ndone\\n```\\n\\nThis script will loop through all items in the current directory and print out the name of each item that is a file.' response_metadata={'token_usage': {'completion_tokens': 81, 'prompt_tokens': 29, 'total_tokens': 110}, 'model_name': 'gpt-4', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-395629ea-fa61-41d3-a3f0-6874b1662d33-0'\n"
     ]
    }
   ],
   "source": [
    "# Now to avoid we need to this every time in a notebook\n",
    "# We can put this in a function in _models\n",
    "\n",
    "# The default model is set in _settings.py\n",
    "import _models, _settings\n",
    "# We print the model name from settings\n",
    "print(_settings.MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We pass this model name to the get_llm helper function\n",
    "llm = _models.get_llm(_settings.MODEL_NAME,callbacks=[callback])\n",
    "result=llm.invoke(challenge_prompt)\n",
    "\n",
    "# We print the final result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_openai\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOpenAI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mChatOpenAI\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_community\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mollama\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChatOllama\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;32mdef\u001b[0m \u001b[0mget_llm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mllm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOpenAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstreaming\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mopenai_llm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChatOpenAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-4\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmax_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmax_retries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# api_key=\"...\",  # if you prefer to pass api key in directly instaed of using env vars\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# base_url=\"...\",\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# organization=\"...\",\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# other params...\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m#if name == \"ollama\":\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mollama_llm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChatOllama\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"llama3\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmax_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmax_retries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# api_key=\"...\",  # if you prefer to pass api key in directly instaed of using env vars\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# base_url=\"...\",\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# organization=\"...\",\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# other params...\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mllm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopenai_llm\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mllm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# For reference you can see the whole helper _models.py file\n",
    "%pycat _models.py "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workshop-agents-DUG816Iy-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
