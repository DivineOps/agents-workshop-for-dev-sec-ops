{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 7 - Forcing the output of a crew\n",
    "In this lesson we have the crew produce structured output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# We first install the packages\n",
    "%pip install crewai langchain langchain-openai langchain-community --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We install our environment variables helper library\n",
    "%pip install python-dotenv --quiet\n",
    "import dotenv\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We load our helper callback to see what goes over the wire\n",
    "from _callbacks import PrettyPrintCallbackHandler\n",
    "callback = PrettyPrintCallbackHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We load the llm mode using our helper and based onthe settings\n",
    "import _models\n",
    "import _settings\n",
    "llm=_models.get_llm(_settings.MODEL_NAME, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We load the llm mode using our helper and based onthe settings\n",
    "import _models\n",
    "import _settings\n",
    "llm=_models.get_llm(_settings.MODEL_NAME, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating data engineer\n"
     ]
    }
   ],
   "source": [
    "from crewai import Agent, Task, Crew, Process\n",
    "import _agents\n",
    "data = _agents.get_agent_by_name(\"data engineer\",llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-15 22:43:45,766 - 8387709952 - __init__.py-__init__:518 - WARNING: Overriding of current TracerProvider is not allowed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m [DEBUG]: == Working Agent: Data Engineer\u001b[00m\n",
      "\u001b[1m\u001b[95m [INFO]: == Starting Task: Extract information from the following text:\n",
      "\n",
      "My name is Patrick Debois . People consider me a DevOps expert and thanks to this workshop I am now also a GenAI expert :).\n",
      "\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
      "[chat_model][start] - prompts : You are Data Engineer. You are very good at extracting data from text.\n",
      "Your personal goal is: Extract the request data from the text.To give my best complete final answer to the task use the exact following format:\n",
      "\n",
      "Thought: I now can give a great answer\n",
      "Final Answer: my best complete final answer to the task.\n",
      "Your final answer must be the great and the most complete as possible, it must be outcome described.\n",
      "\n",
      "I MUST use these formats, my job depends on it!\n",
      "Current Task: Extract information from the following text:\n",
      "\n",
      "My name is Patrick Debois . People consider me a DevOps expert and thanks to this workshop I am now also a GenAI expert :).\n",
      "\n",
      "\n",
      "This is the expect criteria for your final answer: A JSON presentation \n",
      " you MUST return the actual complete content as the final answer, not a summary.\n",
      "\n",
      "Begin! This is VERY important to you, use the tools available and give your best Final Answer, your job depends on it!\n",
      "\n",
      "Thought:\n",
      "\n",
      "\n",
      "=============\n",
      "[llm][end] - generation The text provides information about a person named Patrick Debois. It mentions his expertise in DevOps and GenAI. The information can be extracted and presented in a JSON format.\n",
      "\n",
      "Final Answer: \n",
      "{\n",
      "  \"name\": \"Patrick Debois\",\n",
      "  \"expertise\": [\"DevOps\", \"GenAI\"]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3mThe text provides information about a person named Patrick Debois. It mentions his expertise in DevOps and GenAI. The information can be extracted and presented in a JSON format.\n",
      "\n",
      "Final Answer: \n",
      "{\n",
      "  \"name\": \"Patrick Debois\",\n",
      "  \"expertise\": [\"DevOps\", \"GenAI\"]\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[1m\u001b[92m [DEBUG]: == [Data Engineer] Task output: {\n",
      "  \"first_name\": \"Patrick\",\n",
      "  \"last_name\": \"Debois\",\n",
      "  \"expertise\": [\n",
      "    \"DevOps\",\n",
      "    \"GenAI\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "\u001b[00m\n"
     ]
    }
   ],
   "source": [
    "challenge_prompt = \"\"\"Extract information from the following text:\n",
    "\n",
    "My name is Patrick Debois . People consider me a DevOps expert and thanks to this workshop I am now also a GenAI expert :).\n",
    "\"\"\"\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Person(BaseModel):\n",
    "    first_name: str\n",
    "    last_name: str\n",
    "    expertise: list[str]\n",
    "\n",
    "# Create tasks for your agents\n",
    "task1 = Task(\n",
    "  description=challenge_prompt,\n",
    "  output_json=Person,\n",
    "  agent=data,\n",
    "  llm=llm,\n",
    "  expected_output='A JSON presentation'\n",
    ")\n",
    "\n",
    "# Instantiate your crew with a sequential process\n",
    "crew = Crew(\n",
    "  agents=[data],\n",
    "  tasks=[task1],\n",
    "  verbose=2, # You can set it to 1 or 2 to different logging levels\n",
    ")\n",
    "\n",
    "# Get your crew to work!\n",
    "result = crew.kickoff()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"first_name\": \"Patrick\",\n",
      "  \"last_name\": \"Debois\",\n",
      "  \"expertise\": [\n",
      "    \"DevOps\",\n",
      "    \"GenAI\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "result= task1.output.exported_output\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workshop-agents-DUG816Iy-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
